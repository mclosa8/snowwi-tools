#!/usr/bin/env python3

"""
    Parses the raw PCAP files generated by host computer to raw radar data files.
    Uses the directory time as a reference time. NOTE: This is not robust and should be revisited.
    Adaptation of pcap_2_raw.py for 2026 campaign + Joe's newest update.

    Author(s): Marc Closa Tarres (MCT)
               Joseph Maloyan (JM)
    Date: 2025-02-06
    Version: v0

    Changelog:
        - v0: Initial version. - Feb 06, 2026 - MCT
"""

import numpy as np

import os
import argparse
import time

from datetime import datetime, timezone

from snowwi_tools.lib.file_handling import make_if_not_a_dir

def parse_arguments():
    arg_parser = argparse.ArgumentParser(description='Process PCAP files.')
    arg_parser.add_argument('base_directory', type=str,
                            help='Base directory containing PCAP files')
    arg_parser.add_argument('--save_to', '-st' ,type=str,
                            help='Directory to save output data files')
    arg_parser.add_argument('--pulses-per-file', '-ppf',
                            default=500,
                            type=int,
                            help='Number of pulses per output .dat file. Default: 500.')
    
    args = arg_parser.parse_args()

    args.base_directory = os.path.abspath(args.base_directory)

    return args




def parser_metadata(data0, data1, midnight):
    #tod hhmmss
    tod_hhmmss_0 = data0[4].astype(np.uint16)
    tod_hhmmss_1 = data0[5].astype(np.uint16)
    tod_hhmmss_2 = data0[6].astype(np.uint16)
    ascii_sequence = [tod_hhmmss_0,  tod_hhmmss_1,  tod_hhmmss_2]
    hhmmss = ''.join(chr(num // 256) + chr(num % 256) for num in ascii_sequence)
    #hhmmss
    ss = int(hhmmss[0:2])
    mm = int(hhmmss[2:4])
    hh = int(hhmmss[4:6])
    timeint = hh*60*60 + mm*60 + ss
    #Convert hhmmss to integer. Each is in ASCII
    ts = timeint + midnight
    msb = ts >> 16  # Extract the 16 most significant bits
    lsb = ts & 0xFFFF  # Extract the 16 least significant bits
    return [msb,lsb]




# assign header1 = {tod_drift_in,tod_int_in, 24'b0, voltage_warning, 16'b0, tod_seq_in}; //32,32,24,8,16,16
#     assign header0 = {16'b0, tod_hhmmss_in, 16'b0, burst_counter, global_counter}; //16,48,16,16,32
#     assign m_axis_tdata = use_label ? {header1, header0} : {s_axis_tdata_00, s_axis_tdata_01, s_axis_tdata_10, s_axis_tdata_11};

def find_drops(packet_counts, last_packet_count, num_packets=29309):
    error_indices= []
    #Insert last_packet_count to the beginning of packet_counts
    packet_counts = np.insert(packet_counts, 0, last_packet_count)
    #Invert the function of the packet counter
    pcs = (packet_counts - last_packet_count)//6
    #Take adjacent items in pcs. Take the range between the two items and turn it to a list. Append it to error indices if its not empty
    for i in range(1, len(pcs)):
        diff = pcs[i] - pcs[i-1]
        if diff > 1:
            missing_packets = list(range(packet_counts[i-1]+6, packet_counts[i], 6))
            error_indices.extend(missing_packets)
            print("FOUND DROP:", i)

    return packet_counts[-1], error_indices

def pcap2numpy(pcap_file, last_packet_count, midnight = 0, packets_per_file=29309, using_fcs=True, parse_metaData=True, total_samps = 0):
    pcs = (last_packet_count - 2)//6 + 0
    headerSize = 96
    packets = np.fromfile(pcap_file, dtype=np.uint16)
    packets = packets[12:]
    fcs_tail = 2 if using_fcs else 0
    packets = packets.reshape(-1, 32*53+8+fcs_tail).T[8:-fcs_tail].flatten()
    full_burst_packets = packets.reshape(32*53,-1)
    # print(full_burst_packets.shape)
    full_burst_packets = full_burst_packets
    # print(full_burst_packets.shape)
    # print(offset)

    header = full_burst_packets[:headerSize]
    data = full_burst_packets[headerSize:]
    packet_count_lsb = header[32].astype(np.uint32)
    packet_count_msb = header[33].astype(np.uint32)
    packet_count = packet_count_lsb + (packet_count_msb << 16)
    last_packet_count, error_indices = find_drops(packet_count, last_packet_count, num_packets=29309)
    
    #If error_indices is not empty, insert an zeros array at the indices in error_indices in header and data
    if error_indices:
        print(data.shape)
        print("Correcting for drops")
        print(error_indices, len(error_indices))
        for ei in error_indices:
            eimod = np.mod(ei,packets_per_file)
            header = np.insert(header, eimod, np.zeros(headerSize, dtype=np.uint16), axis=1)
            data = np.insert(data, eimod, np.zeros(1600, dtype=np.uint16), axis=1)
        print(data.shape)

    rows = data.shape[0]
    packetsthisfile = data.shape[1]
    data0_indices = np.hstack([np.arange(i, rows, 32) for i in range(8)])
    data1_indices = np.hstack([np.arange(i, rows, 32) for i in range(8,16)])
    data2_indices = np.hstack([np.arange(i, rows, 32) for i in range(16,24)])
    data3_indices = np.hstack([np.arange(i, rows, 32) for i in range(24,32)])
    data0_indices.sort()
    data1_indices.sort()
    data2_indices.sort()
    data3_indices.sort()
    data0 = data[data0_indices]
    data1 = data[data1_indices]
    data2 = data[data2_indices]
    data3 = data[data3_indices]

    header = header.T.flatten()
    data0 = data0.T.flatten()
    data1 = data1.T.flatten()
    data2 = data2.T.flatten()
    data3 = data3.T.flatten()

    replace_size = 2 # Use 8 for other function    
    num_inserts = 0
    if parse_metaData:
        #Find the nearest multiple of 100000 larger than total_samps
        firstburst = ((pcs + 249) // 250) * 250 - pcs
        burst_start_indices = range(-total_samps % 100000, len(data0), 100000)
        # print("HELLO", burst_start_indices, pcs)
        #For each burst_start_indices, take the first 8 samples from data0 and data1 and pass it to parser_metadata
        for burst_start in burst_start_indices:
            idx = burst_start+num_inserts*2
            metadata = parser_metadata(data0[idx:idx+8], data1[idx:idx+8], midnight)
            #Replace the first 8 samples from burst start from data0,data1,data2,data3 with metadata
            data0[burst_start:burst_start+replace_size] = metadata[0:replace_size]
            data1[burst_start:burst_start+replace_size] = metadata[0:replace_size]
            data2[burst_start:burst_start+replace_size] = metadata[0:replace_size]
            data3[burst_start:burst_start+replace_size] = metadata[0:replace_size]
            # Insert Metadata instead
            # # print(data0.shape)
            # data0 = np.insert(data0,[idx],metadata[0:replace_size])
            # data1 = np.insert(data1,[idx],metadata[0:replace_size])
            # data2 = np.insert(data2,[idx],metadata[0:replace_size])
            # data3 = np.insert(data3,[idx],metadata[0:replace_size])
            # num_inserts += 1
            
    # total_samps += len(data0)
    total_samps += (len(data0) - replace_size*num_inserts)
        
    
    return header, data0, data1, data2, data3, last_packet_count, total_samps, packetsthisfile

def parser(base_directory, save_directory, midnight, pulses_per_file):
    
    # Make output directories
    ch0 = os.path.join(save_directory, 'chan0')
    ch1 = os.path.join(save_directory, 'chan1')
    ch2 = os.path.join(save_directory, 'chan2')
    ch3 = os.path.join(save_directory, 'chan3')
    hds = os.path.join(save_directory, 'headers')

    make_if_not_a_dir(ch0)
    make_if_not_a_dir(ch1)
    make_if_not_a_dir(ch2)
    make_if_not_a_dir(ch3)
    make_if_not_a_dir(hds)


    # This is now here but should be moved somewhere else
    packets_per_file = 29309 #use 29343 if not keeping fcs
    packets_per_burst = 250
    packets_per_new_file = packets_per_burst * pulses_per_file
    samples_per_packet = 8*50
    header_per_packet = 96
    packetsTotal = 0
    parse_metadata = True
    stream0 = []
    stream1 = []
    stream2 = []
    stream3 = []
    streamHeader = []
    j = 0
    last_packet_count = 2
    total_samples = 0
    packetsthisfile = 0
    #num_files is equal to the number of pcap files in base_directory
    num_files = len([f for f in os.listdir(base_directory) if 'pcap' in f])
    num_files = num_files - 1
    print("Number of Files", num_files)
    base_directory = os.path.join(base_directory, "raw_data.pcap")
    for i in range(num_files):
        if i == 0:
            directory = base_directory
        else:
            directory = base_directory + str(i)
        
        header, data0, data1, data2, data3, last_packet_count, total_samples, packetsthisfile = pcap2numpy(directory, last_packet_count, midnight, total_samps = total_samples)
        packetsTotal += packetsthisfile

        if packetsTotal >= packets_per_new_file:
            print("Packets Total Old: ", packetsTotal)
            packetsTotal = packetsTotal % packets_per_new_file
            print("Packets Total New: ", packetsTotal)
            print("Data0 length: ", data0.shape)
            print(packetsTotal * samples_per_packet)
            stream0.append(data0[:-packetsTotal*samples_per_packet])
            stream1.append(data1[:-packetsTotal*samples_per_packet])
            stream2.append(data2[:-packetsTotal*samples_per_packet])
            stream3.append(data3[:-packetsTotal*samples_per_packet])
            streamHeader.append(header[:-packetsTotal*header_per_packet])
            leftover_data0 = data0[-packetsTotal*samples_per_packet:]
            leftover_data1 = data1[-packetsTotal*samples_per_packet:]
            leftover_data2 = data2[-packetsTotal*samples_per_packet:]
            leftover_data3 = data3[-packetsTotal*samples_per_packet:]
            leftover_header = header[-packetsTotal*header_per_packet:]
            # print("Stream0 length: ", np.hstack(stream0).shape, len(stream0), len(stream0[0]), len(stream0[1]), len(stream0[2]), len(stream0[3]), len(stream0[4]))
            #If leftover_data has 0 as the first element in its shape, remove the last element from strea
            if leftover_data0.shape[0] == 0:
                stream0.pop()
                stream1.pop()
                stream2.pop()
                stream3.pop()
                print("Popped data")
            if leftover_header.shape[0] == 0:
                streamHeader.pop()
                print("Popped header")


            #save the data
            print("Saving at iteration: ", i)
            np.hstack(stream0).tofile(os.path.join(ch0, "stream0_" + str(j) + ".dat"))
            np.hstack(stream1).tofile(os.path.join(ch1, "stream1_" + str(j) + ".dat"))
            np.hstack(stream2).tofile(os.path.join(ch2, "stream2_" + str(j) + ".dat"))
            np.hstack(stream3).tofile(os.path.join(ch3, "stream3_" + str(j) + ".dat"))
            np.hstack(streamHeader).tofile(os.path.join(hds, "streamHeaders_" + str(j) + ".dat"))
            j += 1
            if leftover_data0.shape[0] != 0:
                stream0 = [leftover_data0]
                stream1 = [leftover_data1]
                stream2 = [leftover_data2]
                stream3 = [leftover_data3]
            else:
                stream0 = []
                stream1 = []
                stream2 = []
                stream3 = []
            if leftover_header.shape[0] != 0:
                streamHeader = [leftover_header]
            else:
                streamHeader = []
        else:
            stream0.append(data0)
            stream1.append(data1)
            stream2.append(data2)
            stream3.append(data3)
            streamHeader.append(header)

#example usage
#chrt -f 99 python3 pcap2dat.py --base "/home/jon/missioncontrol/in_flight_tools/storage/storage0/stream_1767317565/radar/" --save "/home/jon/data_dump/parsed_toilet/"
#chrt -f 99 python3 ../../post_flight_tools/parsers/pcap2dat.py --base "/home/jon/missioncontrol/in_flight_tools/storage/storage2/stream_1767744016/radar/" --save "/home/jon/data_dump/parsed_toilet/"
def main():
    
    args = parse_arguments()

    parent_directory = os.path.dirname(args.base_directory)

    if args.save_to:
        args.save_to = os.path.abspath(args.save_to)
    else:
        args.save_to = os.path.dirname(args.base_directory)

    print("Data base directory:")
    print(f"    {args.base_directory}\n")

    print("Parent directory:")
    print(f"    {parent_directory}\n")

    print("Saving data to:")
    print(f"    {args.save_to}\n")


    #Take the parent directory (i.e stream_1767317565), if it is in the format "stream_{}", then take the integer 1767317565, and find the date and time
    #Find parent dir by taking the second to last phrase between the two "/"

    parent_directory_basename = os.path.basename(parent_directory)
    print("Basename of parent directory:")
    print(f"    {parent_directory_basename}")

    if parent_directory_basename.startswith("stream_"):
        result = int(parent_directory_basename.split('_')[-1])
        t = datetime.fromtimestamp(result, tz=timezone.utc) 
        midnight_dt = t.replace(hour=0, minute=0, second=18, microsecond=0) #This gets GPS time
        midnight = int(midnight_dt.timestamp())
    else:
        midnight = 0
        t = datetime.fromisoformat('00000000T000000')
    print(midnight)

    # We use ISO 8601 format for directory name.
    output_dirname = f"{t.year}{t.month:02}{t.day:02}T{t.hour:02}{t.minute:02}{t.second:02}"
    output_date = output_dirname.split('T')[0]
    print(output_dirname)
    print(output_date)

    save_directory = os.path.join(args.save_to, output_date, output_dirname)

    print(f"\n\n\nSaving parsed files to {save_directory}")

    start_time = time.time()

    parser(args.base_directory, save_directory, midnight, args.pulses_per_file)

    end_time = time.time()
    
    print(f"Execution time: {end_time - start_time:.2f} seconds")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Script interrupted.")