#!/usr/bin/env python3

"""
    Parses the raw PCAP files generated by host computer to raw radar data files.
    Adaptation from Joseph Maloyan's Jupyter Notebook pcap2dat.ipynb.
    Automatically picks time when 4x2 started recording.

    Author(s): Marc Closa Tarres (MCT)
               Joseph Maloyan
    Date: 2025-02-14
    Version: v0

    Changelog:
        - v0: Initial version. - Feb 14, 2025 - MCT
            - v0.1: Changed directory creation so doesn't need timestamp. - Nov, 05, 2025 - MCT

        - v1: Adaptation to SNOWWI-v2026. New timestamp implemented so it uses GPS time from the data stream and assigns ms recursively. Feb 10, 2026 - MCT
            - v1.1: Included timestamp checking and fixing for any inconsistent timestamping at the beginning of collection (I.e, NMEA message does not show up.). Feb 11, 2026 - MCT
"""

import numpy as np
import pandas as pd

import glob
import os
import sys

import re

from argparse import ArgumentParser
import datetime
from time import sleep

def atoi(text): return int(text) if text.isdigit() else text

def natural_keys(text): return [atoi(c) for c in re.split(r'(\d+)', text)]


def make_if_not_a_dir(path):
    if not os.path.exists(path):
        print(f"Making {path}")
        os.makedirs(path)
    else:
        print(f"{path} already exists.")

# Some constants Joe had hardcoded
packets_per_file = 29309
packets_per_burst = 250
packets_per_new_file = 250 * 500  # Half a second of data
samples_per_packet = 8*50
header_per_packet = 96
# TODO - change this to argument - talk to Joe - Don't really need this anymore
num_files = 1580
packetsTotal = 0

# Global constants
GPS_EPOCH = datetime.datetime(1980, 1, 6, 0, 0, 0)

# Path constants
CWD = os.getcwd()


def parse_args():

    arg_parser = ArgumentParser()

    # TODO - Modify this to accept dynamic paths.
    arg_parser.add_argument('base_directory',
                            help="Path to the PCAP files to process.")
    arg_parser.add_argument('--use-arduino-time', '-at',
                            default=False,
                            action='store_true',
                            help='Uses the arduino time from /base-directory/arduino_dump.log')
    arg_parser.add_argument('--save-to', '-st',
                            help='Path to output directory. Default: /base_directory')
    arg_parser.add_argument('--sec-of-data', '-sd',
                            default='all',
                            help="Total time of radar data to process. Will be converted into number of PCAP files. If 'all', process all the files with data. Default: all.")
    arg_parser.add_argument('--use-gps-time', '-ut',
                        default=False,
                        action='store_true',
                        help='Uses the GPS time from the data stream.')
    arg_parser.add_argument('--no-metadata', '-nmd',
                        default=False,
                        action='store_true',
                        help='Uses the GPS time from the data stream.')

    args = arg_parser.parse_args()
    args.base_directory = os.path.normpath(args.base_directory)
    args.no_metadata = not args.no_metadata

    return args

# Keep Reading the first pulse from each pcap file until a changes in the timestamp is detected

def gettime(data0):
    tod_hhmmss_0 = data0[4].astype(np.uint16)
    tod_hhmmss_1 = data0[5].astype(np.uint16)
    tod_hhmmss_2 = data0[6].astype(np.uint16)
    ##Get tod int
    ascii_sequence = np.array([tod_hhmmss_0,  tod_hhmmss_1,  tod_hhmmss_2])
    hhmmss = (np.stack([(ascii_sequence >> 8) & 0xFF, ascii_sequence & 0xFF], axis=-1).reshape(-1, 6) - ord('0'))[0]
    #hhmmss
    try:
        ss = int(hhmmss[0] * 10 + hhmmss[1])
        mm = int(hhmmss[2] * 10 + hhmmss[3])
        hh = int(hhmmss[4] * 10 + hhmmss[5])
    except:
        ss = 0
        mm = 0
        hh = 0
    ts = hh*60*60 + mm*60 + ss
    return ts


def check_timestamps(pcap_dirs):
        print("Checking for timestamp correctness...")

        limit = 100
        iter = 0
        while iter < limit:
            # print("ITER", iter)
            if iter == 0:
                pcap_file = os.path.join(pcap_dirs, "raw_data.pcap")
            else:
                pcap_file = os.path.join(pcap_dirs, f"raw_data.pcap{iter}")
            headerSize = 96
            packets = np.fromfile(pcap_file, dtype=np.uint16)
            packets = packets[12:]
            fcs_tail = 2
            packets = packets.reshape(-1, 32*53+8+fcs_tail).T[8:-fcs_tail].flatten()
            full_burst_packets = packets.reshape(32*53,-1)

            header_arr = full_burst_packets[:headerSize]
            packet_count_lsb = header_arr[32].astype(np.uint32)
            packet_count_msb = header_arr[33].astype(np.uint32)
            packet_count = packet_count_lsb + (packet_count_msb << 16)
            packet_count = (packet_count - 2)//6
            #Find the first multiple of 250 in packet_count
            multiples_idx = np.where(packet_count % 250 == 0)[0]

            data = full_burst_packets[headerSize:][:8].T

            if iter == 0:
                tod_prev = gettime(data[multiples_idx[0]])
                print(tod_prev)
            for i in multiples_idx:
                tod = gettime(data[i])
                if tod_prev != tod:
                    tod_final = tod - packet_count[i] // 250000 - 1
                    # print(ts, i, packet_count[i])
                    iter += limit
                    break
            iter += 1
        print(tod_final)
        return tod_final


def time_2_num_pcap(time):
    ms_per_pcap = 29309/250
    s_per_pcap = ms_per_pcap / 1000
    print(ms_per_pcap)
    print(s_per_pcap)
    num_files = int(np.ceil(float(time)/s_per_pcap))
    print(num_files)
    return num_files


def datetime_to_gps_timestamp(time):
    return (time - GPS_EPOCH).total_seconds()


def get_arduino_time(directory):
    arduino_file = os.path.join(directory, "..", "logs", "arduino_data.log")
    print(arduino_file)
    log_directory = os.path.normpath(os.path.join(directory, "..", "logs"))
    if not os.path.exists(log_directory):
        print(f"Log directory does not exist: {log_directory}")
        sys.exit(1)
    arduino_df = pd.read_csv(arduino_file, sep='\s+', header=None)
    try:
        first_row = arduino_df.loc[arduino_df[3] == 1].iloc[0]
    except Exception:
        print("Invalid arduino file.")
        sys.exit(1)
    fr_date = first_row[9].split('/')  # Of the format [MM, DD, YYYY]
    fr_time = first_row[10].split(':')  # Of the format [hh, mm, ss]
    print(f"Date from Arduino log: {fr_date}")
    print(f"Time from Arduino log: {fr_time}")
    gps_datetime = datetime.datetime(
        int(fr_date[2]), int(fr_date[0]), int(
            fr_date[1]),  # We want this to be YYYY MM DD
        int(fr_time[0]), int(fr_time[1]), int(fr_time[2]))
    print(gps_datetime)
    arduino_gps_s = datetime_to_gps_timestamp(gps_datetime)
    print(arduino_gps_s)
    # host_unix_s = 0
    # arduino_gps_s = int(datetime.strptime("12/06/2024 01:42:22", "%m/%d/%Y %H:%M:%S").timestamp()) #1733449335
    # host_unix_ms = host_unix_s * 1000
    arduino_unix_ms = arduino_gps_s * 1000
    print(arduino_unix_ms)
    return gps_datetime, arduino_unix_ms


def timestamp_to_64bit(ts):
    return [
        (ts >> 48) & 0xFFFF,
        (ts >> 32) & 0xFFFF,
        (ts >> 16) & 0xFFFF,
        ts & 0xFFFF
    ]


def parser_metadata(data0, data1, midnight, addms=0):
    # assign header1 = {tod_drift_in,tod_int_in, 24'b0, voltage_warning, 16'b0, tod_seq_in}; //32,32,24,8,16,16
    # assign header0 = {16'b0, tod_hhmmss_in, 16'b0, burst_counter, global_counter}; //16,48,16,16,32
    # assign m_axis_tdata = use_label ? {header1, header0} : {s_axis_tdata_00, s_axis_tdata_01, s_axis_tdata_10, s_axis_tdata_11};
    # print(len(data0),len(data0[0]))
    # Tod drift in is the 7,8th samples in data[1]
    drift_lsb = data1[6].astype(np.uint16)
    drift_msb = data1[7].astype(np.uint16)
    # global_counter is the 1 and 2nd sample in data0
    global_counter_lsb = data0[0].astype(np.uint16)
    global_counter_msb = data0[1].astype(np.uint16)
    #burst counter is the 3rd sample in data0
    burst_counter = data0[2].astype(np.uint16)
    # tod_hmmss_in is the 5,6,7th samples in data0
    tod_hhmmss_0 = data0[4].astype(np.uint16)
    tod_hhmmss_1 = data0[5].astype(np.uint16)
    tod_hhmmss_2 = data0[6].astype(np.uint16)
    # voltage warning is the 3rd sample in header1
    voltage_warning = data1[2].astype(np.uint16)
    # tod_seq_in is the first sample in header1
    tod_seq_in = data1[0].astype(np.uint16)
    
    timeint = gettime(data0)
    
    # Add TOD in seconds to midnight.
    ts_dt = midnight + datetime.timedelta(seconds=timeint)
    ts = int(datetime_to_gps_timestamp(ts_dt))

    # Redundant, stores time in seconds as 32bit.
    msb = ts >> 16  # Extract the 16 most significant bits
    lsb = ts & 0xFFFF  # Extract the 16 least significant bits
    # Convert msb and lsb to np.uint16
    msb = np.uint16(msb)
    lsb = np.uint16(lsb)

    # Get time as integer in ms(np.uint64), then split into 4 16 bit chunks
    ts64 = int(ts)
    ts64 = ts64 * 1000
    
    ts64_chunk0, ts64_chunk1, ts64_chunk2, ts64_chunk3 = timestamp_to_64bit(ts64)

    # Return all items in a list
    return [np.uint64(ts64), np.uint16(ts64_chunk0), np.uint16(ts64_chunk1), np.uint16(ts64_chunk2), np.uint16(ts64_chunk3), 
            np.uint16(drift_lsb), np.uint16(drift_msb), np.uint16(msb), np.uint16(lsb)]


def find_drops(packet_counts, last_packet_count, num_packets=29309):
    error_indices= []
    #Insert last_packet_count to the beginning of packet_counts
    packet_counts = np.insert(packet_counts, 0, last_packet_count)
    #Invert the function of the packet counter
    pcs = (packet_counts - last_packet_count) // 6
    # Take adjacent items in pcs. Take the range between the two items and turn it to a list. Append it to error indices if its not empty
    for i in range(1, len(pcs)):
        diff = pcs[i] - pcs[i-1]
        if diff > 1:
            missing_packets = list(range(packet_counts[i-1]+6, packet_counts[i], 6))
            error_indices.extend(missing_packets)
            print("FOUND DROP:", i)

    return packet_counts[-1], error_indices


def pcap2numpy(pcap_file, last_packet_count, midnight, packets_per_file=29309, using_fcs=True, parse_metaData=True, total_samps = 0):
    pcs = (last_packet_count - 2)//6 + 0
    headerSize = 96
    packets = np.fromfile(pcap_file, dtype=np.uint16)
    packets = packets[12:]
    fcs_tail = 2 if using_fcs else 0
    # packets = packets.reshape(-1, 32*53+8+fcs_tail).T[8:-fcs_tail].flatten()
    packets = packets.reshape(-1, 32*53+8+fcs_tail).T
    packets = np.ascontiguousarray(packets[8:-fcs_tail]).ravel()
    full_burst_packets = packets.reshape(32*53,-1)
    # print(full_burst_packets.shape)
    full_burst_packets = full_burst_packets
    # print(full_burst_packets.shape)
    # print(offset)

    header = full_burst_packets[:headerSize]
    data = full_burst_packets[headerSize:]
    packet_count_lsb = header[32].astype(np.uint32)
    packet_count_msb = header[33].astype(np.uint32)
    packet_count = packet_count_lsb + (packet_count_msb << 16)
    last_packet_count, error_indices = find_drops(packet_count, last_packet_count, num_packets=29309)
    
    # If error_indices is not empty, insert an zeros array at the indices in error_indices in header and data
    if error_indices:
        print(data.shape)
        print("Correcting for drops")
        print(error_indices, len(error_indices))
        for ei in error_indices:
            eimod = np.mod(ei,packets_per_file)
            header = np.insert(header, eimod, np.zeros(headerSize, dtype=np.uint16), axis=1)
            data = np.insert(data, eimod, np.zeros(1600, dtype=np.uint16), axis=1)
        print(data.shape)

    rows = data.shape[0]
    packetsthisfile = data.shape[1]
    data0_indices = np.hstack([np.arange(i, rows, 32) for i in range(8)])
    data1_indices = np.hstack([np.arange(i, rows, 32) for i in range(8,16)])
    data2_indices = np.hstack([np.arange(i, rows, 32) for i in range(16,24)])
    data3_indices = np.hstack([np.arange(i, rows, 32) for i in range(24,32)])
    data0_indices.sort()
    data1_indices.sort()
    data2_indices.sort()
    data3_indices.sort()
    data0 = data[data0_indices]
    data1 = data[data1_indices]
    data2 = data[data2_indices]
    data3 = data[data3_indices]

    # header = header.T.flatten()
    # data0 = data0.T.flatten()
    # data1 = data1.T.flatten()
    # data2 = data2.T.flatten()
    # data3 = data3.T.flatten()

    header = np.ascontiguousarray(header.T).ravel()
    data0 = np.ascontiguousarray(data0.T).ravel()
    data1 = np.ascontiguousarray(data1.T).ravel()
    data2 = np.ascontiguousarray(data2.T).ravel()
    data3 = np.ascontiguousarray(data3.T).ravel()

    if parse_metaData:
        #Find the nearest multiple of 100000 larger than total_samps
        firstburst = ((pcs + 249) // 250) * 250 - pcs
        burst_start_indices = range(-total_samps % 100000, len(data0), 100000)
        # print("HELLO", burst_start_indices, pcs)
        #For each burst_start_indices, take the first 8 samples from data0 and data1 and pass it to parser_metadata
        for burst_start in burst_start_indices:
            # Force copies
            metadata = np.array([m.copy() for m in parser_metadata(data0[burst_start:burst_start+8], data1[burst_start:burst_start+8], midnight, 0)])
            
            # Replace first 4 samples with time with metadata(get time whenn addms = 0)
            # if addms == 0:
            #     ts = metadata[0:4]
            # elif addms == 1000:
            #     addms = 0
            #     ts = metadata[0:4]
            # else:
            #     ts = metadata[0:4]
            # # print(data0[:10])
            # data0[burst_start+0:burst_start+4] = ts
            # data1[burst_start+0:burst_start+4] = ts
            # data2[burst_start+0:burst_start+4] = ts
            # data3[burst_start+0:burst_start+4] = ts
            # #Replace the last 4 samples from burst start from data0,data1,data2,data3 with metadata
            # data0[burst_start+4:burst_start+8] = metadata[4:8]
            # data1[burst_start+4:burst_start+8] = metadata[4:8]
            # data2[burst_start+4:burst_start+8] = metadata[4:8]
            # data3[burst_start+4:burst_start+8] = metadata[4:8]
            # # print(data0[:10], '\n')
            # addms += 1
            
    total_samps += len(data0)
        
    
    return header, data0, data1, data2, data3, last_packet_count, total_samps, packetsthisfile, metadata


def main():
    #Couple of placeholders for new parsing stuff
    # midnight = 0

    args = parse_args()

    print(args)

    parent_directory = os.path.dirname(args.base_directory)
    base_name = os.path.basename(parent_directory)
    print(base_name)

    filelist = glob.glob(os.path.join(args.base_directory, "*.pcap*"))
    if len(filelist) == 0:
        args.base_directory = os.path.join(args.base_directory, base_name)
        print(args.base_directory)
        filelist = glob.glob(os.path.join(args.base_directory, "*.pcap*"))
    filelist.sort(key=natural_keys)
    print(len(filelist))

    # If arduino time definition - if arduino not used, uses time from directory name
    if args.use_arduino_time:
        time, arduino_unix_ms = get_arduino_time(args.base_directory)
    else:
        try:
            time_from_dir = base_name.split("_")[-1]
            time = datetime.datetime.utcfromtimestamp(int(time_from_dir))
        except ValueError:
            time = datetime.datetime(year=1999, month=1, day=1)

    print(time)
    dir_name = f"{time.year}{time.month:02}{time.day:02}T{time.hour:02}{time.minute:02}{time.second:02}"
    day_dir = dir_name.split('T')[0]
    print(dir_name)
    print(day_dir)
    midnight = datetime.datetime(year=time.year, month=time.month, day=time.day)
    print(midnight)

    # Define output directories
    if not args.save_to:
        save_directory = args.base_directory
    else:
        save_directory = args.save_to

    save_directory = os.path.join(save_directory, day_dir, dir_name)

    if not args.use_gps_time:
        save_directory += '_no_gps_time'
    print(f"\n\n\nSaving parsed files to {save_directory}")

    # Make output directories
    ch0 = os.path.join(save_directory, 'chan0')
    ch1 = os.path.join(save_directory, 'chan1')
    ch2 = os.path.join(save_directory, 'chan2')
    ch3 = os.path.join(save_directory, 'chan3')
    hds = os.path.join(save_directory, 'headers')

    make_if_not_a_dir(ch0)
    make_if_not_a_dir(ch1)
    make_if_not_a_dir(ch2)
    make_if_not_a_dir(ch3)
    make_if_not_a_dir(hds)

    if args.sec_of_data.isdigit():
        num_files = time_2_num_pcap(args.sec_of_data)
        filelist = filelist[:int(num_files)]
    elif args.sec_of_data.lower() == 'all':
        filelist = [file for file in filelist if os.path.getsize(file) > 0]
    else:
        print("Invalid number of files.")
        sys.exit(1)

    num_files = len(filelist)

    print(f"Processing {num_files} files, starting from file")
    print(f"    {filelist[0]}")
    print("to file")
    print(f"    {filelist[-1]}")
    print("\n\n\n")
    sleep(3)

    # Definition of streams
    stream0 = []
    stream1 = []
    stream2 = []
    stream3 = []
    streamHeader = []

    # Check the timestamps for correctness
    tod_s = check_timestamps(args.base_directory)
    tod_datetime = midnight + datetime.timedelta(seconds=float(tod_s))
    ts_ms = int(datetime_to_gps_timestamp(tod_datetime)) * 1000
    print(ts_ms)

    # Output file
    j = 0
    last_packet_count = 2
    total_samples = 0
    packetsthisfile = 0
    packetsTotal = 0
    for i, file in enumerate(filelist):
        
        print(file)

        packeted = pcap2numpy(file, last_packet_count, midnight, total_samps=total_samples, parse_metaData=args.no_metadata)
        header, data0, data1, data2, data3, last_packet_count, total_samples, packetsthisfile, metadata = packeted
        packetsTotal += packetsthisfile

        # First file, get time and extrapolate - NOTE: First timestamp is derived when checking for correctness and used from there.
        # if i == 0:
        #     ts_ms = metadata[0]
        #     print(ts_ms)

        if packetsTotal >= packets_per_new_file:
            print("Packets Total Old: ", packetsTotal)
            packetsTotal = packetsTotal % packets_per_new_file
            print("Packets Total New: ", packetsTotal)
            print("Data0 length: ", data0.shape)
            print(packetsTotal * samples_per_packet)
            stream0.append(data0[:-packetsTotal*samples_per_packet])
            stream1.append(data1[:-packetsTotal*samples_per_packet])
            stream2.append(data2[:-packetsTotal*samples_per_packet])
            stream3.append(data3[:-packetsTotal*samples_per_packet])
            streamHeader.append(header[:-packetsTotal*header_per_packet])
            leftover_data0 = data0[-packetsTotal*samples_per_packet:]
            leftover_data1 = data1[-packetsTotal*samples_per_packet:]
            leftover_data2 = data2[-packetsTotal*samples_per_packet:]
            leftover_data3 = data3[-packetsTotal*samples_per_packet:]
            leftover_header = header[-packetsTotal*header_per_packet:]
            # If leftover_data has 0 as the first element in its shape, remove the last element from strea
            if leftover_data0.shape[0] == 0:
                stream0.pop()
                stream1.pop()
                stream2.pop()
                stream3.pop()
                print("Popped data")
            if leftover_header.shape[0] == 0:
                streamHeader.pop()
                print("Popped header")

            # Save the data
            if not args.use_gps_time:
                print("Saving at iteration: ", i)
                np.hstack(stream0).tofile(os.path.join(
                    ch0, "stream0_" + str(j) + ".dat"))
                np.hstack(stream1).tofile(os.path.join(
                    ch1, "stream1_" + str(j) + ".dat"))
                np.hstack(stream2).tofile(os.path.join(
                    ch2, "stream2_" + str(j) + ".dat"))
                np.hstack(stream3).tofile(os.path.join(
                    ch3, "stream3_" + str(j) + ".dat"))
                np.hstack(streamHeader).tofile(os.path.join(
                    hds, "streamHeader_" + str(j) + ".dat"))
            else:
                stream0 = np.hstack(stream0).flatten()
                stream1 = np.hstack(stream1).flatten()
                stream2 = np.hstack(stream2).flatten()
                stream3 = np.hstack(stream3).flatten()
                streamHeader = np.hstack(streamHeader).flatten()
                times = np.linspace(
                    ts_ms + j*500, ts_ms + 499 + j*500, 500).astype(np.uint64)
                shift_amounts = np.array([48, 32, 16, 0], dtype=np.uint64)
                print(times.min(), times.max())

                insert_arrays = ((times[:, None] >> shift_amounts) & np.uint64(0xFFFF)).astype(np.uint16)

                stream0 = stream0.reshape(-1, 100000)
                stream1 = stream1.reshape(-1, 100000)
                stream2 = stream2.reshape(-1, 100000)
                stream3 = stream3.reshape(-1, 100000)
                streamHeader = streamHeader.reshape(-1, 100000)

                # THIS IS A FIX FOR SOME LEFTOVER DATA
                # First 8 data samples in Chan0 and chan1 are replaced by metadata.
                # Will be set to zero to not interfere with range compression.
                # Ch2 and Ch3 also set to zero for consistency.

                stream0[:8] = 0
                stream1[:8] = 0
                stream2[:8] = 0
                stream3[:8] = 0

                stream0 = np.hstack([insert_arrays, stream0])
                stream1 = np.hstack([insert_arrays, stream1])
                stream2 = np.hstack([insert_arrays, stream2])
                stream3 = np.hstack([insert_arrays, stream3])
                
                print("Saving at iteration: ", i)
                stream0.tofile(os.path.join(
                    ch0, "stream0_" + str(j) + ".dat"))
                stream1.tofile(os.path.join(
                    ch1, "stream1_" + str(j) + ".dat"))
                stream2.tofile(os.path.join(
                    ch2, "stream2_" + str(j) + ".dat"))
                stream3.tofile(os.path.join(
                    ch3, "stream3_" + str(j) + ".dat"))
                streamHeader.tofile(os.path.join(
                    hds, "streamHeader" + str(j) + ".dat"))

            j += 1
            if leftover_data0.shape[0] != 0:
                stream0 = [leftover_data0]
                stream1 = [leftover_data1]
                stream2 = [leftover_data2]
                stream3 = [leftover_data3]
            else:
                stream0 = []
                stream1 = []
                stream2 = []
                stream3 = []
            if leftover_header.shape[0] != 0:
                streamHeader = [leftover_header]
            else:
                streamHeader = []
        else:
            stream0.append(data0)
            stream1.append(data1)
            stream2.append(data2)
            stream3.append(data3)
            streamHeader.append(header)


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("Script interrupted.")
